<h1>Feature: Features could be tagged</h1>
<p>For picking up tests to run we can use
<code>tests selection &lt;http://pytest.org/latest/usage.html#specifying-tests-selecting-tests&gt;</code>_ technique.
The problem is that you have to know how your tests are organized,
knowing only the feature files organization is not enough.
<code>cucumber tags &lt;https://github.com/cucumber/cucumber/wiki/Tags&gt;</code>_ introduces standard way of
categorizing your features and scenarios</p>
<h2>Rule:</h2>
<h3>Background:</h3>
<ul>
<li>Given File &quot;Passed.feature&quot; with content:
<pre><code class="language-gherkin">@passed
Feature: Steps are executed by corresponding step keyword decorator
  Scenario: Passed
    Given I produce passed test
</code></pre>
</li>
<li>Given File &quot;Failed.feature&quot; with content:
<pre><code class="language-gherkin">@failed
Feature: Steps are executed by corresponding step keyword decorator
  Scenario: Failed
    Given I produce failed test
</code></pre>
</li>
<li>Given File &quot;Both.feature&quot; with content:
<pre><code class="language-gherkin">@both
Feature: Steps are executed by corresponding step keyword decorator
  Scenario: Passed
    Given I produce passed test

  Scenario: Failed
    Given I produce failed test
</code></pre>
</li>
<li>Given File &quot;pytest.ini&quot; with content:
<pre><code class="language-ini">[pytest]
markers =
  passed
  failed
  both
</code></pre>
</li>
<li>And File &quot;conftest.py&quot; with content:
<pre><code class="language-python">from pytest_bdd.compatibility.pytest import fail
from pytest_bdd import given

@given('I produce passed test')
def passing_step():
  ...

@given('I produce failed test')
def failing_step():
  fail('Enforce fail')
</code></pre>
</li>
</ul>
<h3>Scenario:</h3>
<ul>
<li>
<p>When run pytest</p>
<table>
<thead>
<tr>
<th>cli_args</th>
<th>-m</th>
<th>passed</th>
</tr>
</thead>
</table>
</li>
<li>
<p>Then pytest outcome must contain tests with statuses:</p>
<table>
<thead>
<tr>
<th>passed</th>
<th>failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h3>Scenario:</h3>
<ul>
<li>
<p>When run pytest</p>
<table>
<thead>
<tr>
<th>cli_args</th>
<th>-m</th>
<th>failed</th>
</tr>
</thead>
</table>
</li>
<li>
<p>Then pytest outcome must contain tests with statuses:</p>
<table>
<thead>
<tr>
<th>passed</th>
<th>failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h3>Scenario:</h3>
<ul>
<li>
<p>When run pytest</p>
<table>
<thead>
<tr>
<th>cli_args</th>
<th>-m</th>
<th>passed or failed</th>
</tr>
</thead>
</table>
</li>
<li>
<p>Then pytest outcome must contain tests with statuses:</p>
<table>
<thead>
<tr>
<th>passed</th>
<th>failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h3>Scenario:</h3>
<ul>
<li>
<p>When run pytest</p>
<table>
<thead>
<tr>
<th>cli_args</th>
<th>-m</th>
<th>not both</th>
</tr>
</thead>
</table>
</li>
<li>
<p>Then pytest outcome must contain tests with statuses:</p>
<table>
<thead>
<tr>
<th>passed</th>
<th>failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h3>Scenario:</h3>
<ul>
<li>
<p>When run pytest</p>
<table>
<thead>
<tr>
<th>cli_args</th>
<th>-m</th>
<th>both</th>
</tr>
</thead>
</table>
</li>
<li>
<p>Then pytest outcome must contain tests with statuses:</p>
<table>
<thead>
<tr>
<th>passed</th>
<th>failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h3>Scenario:</h3>
<ul>
<li>When run pytest</li>
<li>Then pytest outcome must contain tests with statuses:
<table>
<thead>
<tr>
<th>passed</th>
<th>failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
</li>
</ul>
